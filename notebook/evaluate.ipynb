{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0cd5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial, stats\n",
    "\n",
    "from models import (\n",
    "    SupervisedSimCSE,\n",
    "    UnsupervisedSimCSE,\n",
    "    PrefixSupervisedSimCSE,\n",
    "    SupervisedCPT,\n",
    "    UnsupervisedCPT,\n",
    "    PrefixSupervisedCPT,\n",
    "    PrefixUnsupervisedCPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f10e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device=torch.device(\"cuda:3\")\n",
    "\n",
    "# Load Pre-Trained Tokenizer, LM\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "pretrained=AutoModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Add Pad Token: [PAD]\n",
    "if tokenizer.pad_token==None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    pretrained.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load Trained Model: SimCSE\n",
    "# model=SupervisedSimCSE(pretrained=pretrained)\n",
    "# model=UnsupervisedSimCSE(pretrained=pretrained)\n",
    "\n",
    "# Load Trained Model: SimCSE with Prefix-Tuning\n",
    "# model=PrefixSupervisedSimCSE(base_config=pretrained.config, preseqlen=5, hidden_dim=512)\n",
    "\n",
    "# Load Trained Model: CPT\n",
    "# model=SupervisedCPT(pretrained=pretrained)\n",
    "# model=UnsupervisedCPT(pretrained=pretrained)\n",
    "\n",
    "# Load Trained Model: CPT with Prefix-Tuning\n",
    "#model=PrefixSupervisedCPT(base_config=pretrained.config, preseqlen=5, hidden_dim=512)\n",
    "model=PrefixUnsupervisedCPT(base_config=pretrained.config, preseqlen=5, hidden_dim=512)\n",
    "\n",
    "model.load_state_dict(torch.load(\"../model/cpt-unsup-prefix(gpt2)_preseqlen5_hidden512_batch192_lr0.0001_step6500.pth\"))\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023657b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STS Benchmark Dataset\n",
    "# https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark\n",
    "with open(\"./dataset/stsbenchmark/sts-test.csv\", \"r\") as f:\n",
    "    stsb_test=f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7bca301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Mode\n",
    "pretrained.eval()\n",
    "model.eval()\n",
    "\n",
    "preds=[]\n",
    "labels=[]\n",
    "for data in stsb_test.split('\\n')[:-1]:\n",
    "    label, sent1, sent2=data.split('\\t')[4:7]\n",
    "    labels.append(float(label))\n",
    "    \n",
    "    # General Setting (without Prefix-Tuning)\n",
    "#     repr_sent1=model.get_embedding(tokenizer.encode(sent1, return_tensors='pt').to(device))\n",
    "#     repr_sent2=model.get_embedding(tokenizer.encode(sent2, return_tensors='pt').to(device))\n",
    "    \n",
    "    # with Prefix-Tuning\n",
    "    repr_sent1=model.get_embedding(\n",
    "        pretrained=pretrained,\n",
    "        x=tokenizer.encode(sent1, return_tensors='pt').to(device)\n",
    "    )\n",
    "    repr_sent2=model.get_embedding(\n",
    "        pretrained=pretrained,\n",
    "        x=tokenizer.encode(sent2, return_tensors='pt').to(device)\n",
    "    )\n",
    "    \n",
    "    pred=1-spatial.distance.cosine(np.array(repr_sent1.detach().cpu()), np.array(repr_sent2.detach().cpu()))\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f0fff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.69801416],\n",
       "       [0.69801416, 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca899d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6908329011074216, pvalue=3.451984920212133e-196)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0626e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
