{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aeeaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67190640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device=torch.device('cuda:3')\n",
    "# Hyperparams\n",
    "epochs=3\n",
    "\"\"\"\n",
    "Due to In-Batch Negative Samples,\n",
    "batch_size 32 * accum_steps 16 = 512 is NOT SAME with\n",
    "batch_size 512 * accum_steps 1 = 512 (Setting in Paper).\n",
    "\"\"\"\n",
    "batch_size=32\n",
    "accum_steps=16\n",
    "lr=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80842cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pre-Trained Tokenizer\n",
    "tokenizer=RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# Pre-Trained LM\n",
    "pretrained=RobertaModel.from_pretrained(\"roberta-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c93b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    NLI Dataset for Supervised SimCSE\n",
    "    \"\"\"\n",
    "    def __init__(self, path, tokenizer):\n",
    "        # Triplet: Sentence, Positive (Entailment), Hard Negative (Contradiction)\n",
    "        self.sent=[]\n",
    "        self.pos=[]\n",
    "        self.neg=[]\n",
    "        \n",
    "        # Load Data\n",
    "        df_nli=pd.read_csv(path)\n",
    "        \n",
    "        for index in df_nli.index:\n",
    "            data=df_nli.loc[index]\n",
    "            \n",
    "            self.sent.append(tokenizer.encode(data['sent0']))\n",
    "            self.pos.append(tokenizer.encode(data['sent1']))\n",
    "            self.neg.append(tokenizer.encode(data['hard_neg']))\n",
    "            \n",
    "        print(len(self.sent), 'data')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sent[idx], self.pos[idx], self.neg[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854b7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Same Sequence Length on Same Batch\n",
    "    \"\"\"\n",
    "    max_len_sent=0\n",
    "    max_len_pos=0\n",
    "    max_len_neg=0\n",
    "    for sent, pos, neg in batch:\n",
    "        if len(sent)>max_len_sent: max_len_sent=len(sent)\n",
    "        if len(pos)>max_len_pos: max_len_pos=len(pos)\n",
    "        if len(neg)>max_len_neg: max_len_neg=len(neg)\n",
    "            \n",
    "    batch_sent=[]\n",
    "    batch_pos=[]\n",
    "    batch_neg=[]\n",
    "    for sent, pos, neg in batch:\n",
    "        sent.extend([tokenizer.pad_token_id]*(max_len_sent-len(sent)))\n",
    "        batch_sent.append(sent)\n",
    "        \n",
    "        pos.extend([tokenizer.pad_token_id]*(max_len_pos-len(pos)))\n",
    "        batch_pos.append(pos)\n",
    "        \n",
    "        neg.extend([tokenizer.pad_token_id]*(max_len_neg-len(neg)))\n",
    "        batch_neg.append(neg)\n",
    "        \n",
    "    return torch.tensor(batch_sent), torch.tensor(batch_pos), torch.tensor(batch_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdb4cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275601 data\n"
     ]
    }
   ],
   "source": [
    "# Supervised Dataset from Official GitHub\n",
    "# https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/nli_for_simcse.csv\n",
    "dataset_train=NLIDataset(path='../dataset/nli_for_simcse.csv', tokenizer=tokenizer)\n",
    "dataloader_train=DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b51ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised SimCSE\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        self.cos_sim=nn.CosineSimilarity(dim=-1)\n",
    "        # Temperature\n",
    "        self.temp=0.05\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def pooler(self, x):\n",
    "        # [CLS] without MLP\n",
    "        return x.last_hidden_state[:,0,:]\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        # Return Sentence Representation\n",
    "        x=self.pretrained(x)\n",
    "        return self.pooler(x)\n",
    "    \n",
    "    def forward(self, sent, pos, neg):\n",
    "        # Forward\n",
    "        sent=self.pretrained(sent)\n",
    "        pos=self.pretrained(pos)\n",
    "        neg=self.pretrained(neg)\n",
    "        \n",
    "        # Pooling\n",
    "        repr_sent=self.pooler(sent)\n",
    "        repr_pos=self.pooler(pos)\n",
    "        repr_neg=self.pooler(neg)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        sim_pos=self.cos_sim(repr_sent.unsqueeze(1), repr_pos.unsqueeze(0))/self.temp\n",
    "        sim_neg=self.cos_sim(repr_sent.unsqueeze(1), repr_neg.unsqueeze(0))/self.temp\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        sim=torch.cat([sim_pos, sim_neg], dim=1)\n",
    "        label=torch.arange(sim.size(0)).long().to(sim.device)\n",
    "        loss=self.loss(sim, label)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d71455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "# Model: Supervised SimCSE\n",
    "model=SimCSE(pretrained=pretrained).to(device)\n",
    "model.train()\n",
    "\n",
    "# Optimizer, Scheduler\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=int(epochs*len(dataset_train)/(accum_steps*batch_size))\n",
    ")\n",
    "\n",
    "# Mixed Precision: GradScaler\n",
    "scaler=amp.GradScaler()\n",
    "\n",
    "# Tensorboard\n",
    "writer=SummaryWriter()\n",
    "\n",
    "step_global=0\n",
    "for epoch in range(epochs):\n",
    "    loss_=0\n",
    "    optimizer.zero_grad()\n",
    "    for step, (sent, pos, neg) in enumerate(dataloader_train):\n",
    "        # Load on Device\n",
    "        sent=sent.to(device)\n",
    "        pos=pos.to(device)\n",
    "        neg=neg.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        with amp.autocast():\n",
    "            loss=model(sent, pos, neg)\n",
    "            loss=loss/accum_steps\n",
    "        # Backward\n",
    "        scaler.scale(loss).backward()\n",
    "        loss_+=loss.item()\n",
    "        \n",
    "        # Step\n",
    "        if (step+1)%accum_steps==0:\n",
    "            step_global+=1\n",
    "            \n",
    "            # Tensorboard\n",
    "            writer.add_scalar(\n",
    "                f'loss_train/SimCSE_Sup_batch{int(accum_steps*batch_size)}_lr{lr}_epochs{epochs}',\n",
    "                loss_,\n",
    "                step_global\n",
    "            )\n",
    "            loss_=0\n",
    "            \n",
    "            # Optimizer, Scheduler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if (step_global+1)%250==0:\n",
    "                # Eval Phase\n",
    "                # Save Model\n",
    "                model.to(torch.device('cpu'))\n",
    "                torch.save(\n",
    "                    model,\n",
    "                    f'../model/SimCSE_Sup_batch{int(accum_steps*batch_size)}_lr{lr}_step{step_global+1}'\n",
    "                )\n",
    "                model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b918801",
   "metadata": {},
   "source": [
    "### Evaluation on STS-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1669c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e227837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Supervised SimCSE\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        self.cos_sim=nn.CosineSimilarity(dim=-1)\n",
    "        # Temperature\n",
    "        self.temp=0.05\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def pooler(self, x):\n",
    "        # [CLS] without MLP\n",
    "        return x.last_hidden_state[:,0,:]\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        x=self.pretrained(x)\n",
    "        return self.pooler(x)\n",
    "    \n",
    "    def forward(self, sent, pos, neg):\n",
    "        # Forward\n",
    "        sent=self.pretrained(sent)\n",
    "        pos=self.pretrained(pos)\n",
    "        neg=self.pretrained(neg)\n",
    "        \n",
    "        # Pooling\n",
    "        repr_sent=self.pooler(sent)\n",
    "        repr_pos=self.pooler(pos)\n",
    "        repr_neg=self.pooler(neg)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        sim_pos=self.cos_sim(repr_sent.unsqueeze(1), repr_pos.unsqueeze(0))/self.temp\n",
    "        sim_neg=self.cos_sim(repr_sent.unsqueeze(1), repr_neg.unsqueeze(0))/self.temp\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        sim=torch.cat([sim_pos, sim_neg], dim=1)\n",
    "        label=torch.arange(sim.size(0)).long().to(sim.device)\n",
    "        loss=self.loss(sim, label)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a3637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device=torch.device('cuda:3')\n",
    "\n",
    "# Pre-Trained Tokenizer\n",
    "tokenizer=RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load Trained Model: Supervised SimCSE\n",
    "model=torch.load('../model/SimCSE_Sup_batch512_lr5e-05_step250').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bf70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STS Benchmark Dataset\n",
    "# https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark\n",
    "with open('../dataset/stsbenchmark/sts-test.csv', 'r') as f:\n",
    "    stsb_test=f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4069287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Mode\n",
    "model.eval()\n",
    "\n",
    "preds=[]\n",
    "labels=[]\n",
    "for data in stsb_test.split('\\n')[:-1]:\n",
    "    label, sent1, sent2=data.split('\\t')[4:7]\n",
    "    labels.append(float(label))\n",
    "    \n",
    "    repr_sent1=model.get_embedding(tokenizer.encode(sent1, return_tensors='pt').to(device))\n",
    "    repr_sent2=model.get_embedding(tokenizer.encode(sent2, return_tensors='pt').to(device))\n",
    "    \n",
    "    pred=1-spatial.distance.cosine(np.array(repr_sent1.detach().cpu()), np.array(repr_sent2.detach().cpu()))\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987599a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.8337637],\n",
       "       [0.8337637, 1.       ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef18ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.8380499314067523, pvalue=0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da4a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
