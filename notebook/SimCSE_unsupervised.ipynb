{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1633bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7f5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device=torch.device(\"cuda:3\")\n",
    "# Hyperparams\n",
    "epochs=1\n",
    "\"\"\"\n",
    "Due to In-Batch Negative Samples,\n",
    "batch_size 16 * accum_steps 8 = 128 is NOT SAME with\n",
    "batch_size 128 * accum_steps 1 = 128.\n",
    "\"\"\"\n",
    "batch_size=16\n",
    "accum_steps=8\n",
    "lr=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d37df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pre-Trained Tokenizer\n",
    "tokenizer=RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# Pre-Trained LM\n",
    "pretrained=RobertaModel.from_pretrained(\"roberta-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ef7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Randomly Sampled Wikipedia-Sentences for Unsupervised SimCSE\n",
    "    \"\"\"\n",
    "    def __init__(self, path, tokenizer):\n",
    "        # Pair: Sentence, Positive (=Sentence)\n",
    "        self.sent=[]\n",
    "        self.pos=[]\n",
    "        \n",
    "        # Load Data\n",
    "        df_wiki=pd.read_csv(path)\n",
    "        \n",
    "        for index in df_wiki.index:\n",
    "            data=df_wiki.loc[index]\n",
    "            \n",
    "            # Encode\n",
    "            enc0=tokenizer.encode(data[\"sent0\"])\n",
    "            # Truncate\n",
    "            if len(enc0)>512:\n",
    "                enc0=enc0[:511]+[tokenizer.eos_token_id]\n",
    "            # Append\n",
    "            self.sent.append(enc0)\n",
    "            \n",
    "            # Encode\n",
    "#             enc1=tokenizer.encode(data[\"sent1\"])\n",
    "            # Truncate\n",
    "#             if len(enc1)>512:\n",
    "#                 enc1=enc1[:511]+[tokenizer.eos_token_id]\n",
    "            # Append\n",
    "            self.pos.append(enc0)\n",
    "            \n",
    "        print(len(self.sent), \"data\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sent[idx], self.pos[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b16c39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Same Sequence Length on Same Batch\n",
    "    \"\"\"\n",
    "    max_len_sent=0\n",
    "    max_len_pos=0\n",
    "    for sent, pos in batch:\n",
    "        if len(sent)>max_len_sent: max_len_sent=len(sent)\n",
    "        if len(pos)>max_len_pos: max_len_pos=len(pos)\n",
    "            \n",
    "    batch_sent=[]\n",
    "    batch_pos=[]\n",
    "    for sent, pos in batch:\n",
    "        sent.extend([tokenizer.pad_token_id]*(max_len_sent-len(sent)))\n",
    "        batch_sent.append(sent)\n",
    "        \n",
    "        pos.extend([tokenizer.pad_token_id]*(max_len_pos-len(pos)))\n",
    "        batch_pos.append(pos)\n",
    "        \n",
    "    return torch.tensor(batch_sent), torch.tensor(batch_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9056def4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 data\n"
     ]
    }
   ],
   "source": [
    "# Unsupervised Dataset from Official GitHub\n",
    "# https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt\n",
    "with open(\"../dataset/wiki1m_for_simcse.txt\", \"r\") as f:\n",
    "    data=f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "data.remove(\"\")\n",
    "print(len(data), \"data\")\n",
    "\n",
    "# Save Dataset as CSV File\n",
    "pd.DataFrame({\"sent0\": data, \"sent1\": data}).to_csv(\"../dataset/wiki1m_for_simcse.csv\")\n",
    "# Load Dataset, DataLoader\n",
    "dataset_train=WikiDataset(path=\"../dataset/wiki1m_for_simcse.csv\", tokenizer=tokenizer)\n",
    "dataloader_train=DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05def5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Unsupervised SimCSE\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        # Pooling Layer: MLP (Train Only)\n",
    "        self.mlp=nn.Linear(self.pretrained.config.hidden_size, self.pretrained.config.hidden_size)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        self.cos_sim=nn.CosineSimilarity(dim=-1)\n",
    "        # Temperature\n",
    "        self.temp=0.05\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def pooler(self, x):\n",
    "        # [CLS] with MLP (Train Only)\n",
    "        x=x.last_hidden_state[:,0,:]\n",
    "        return self.mlp(x)\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        # Return Sentence Representation\n",
    "        x=self.pretrained(x)\n",
    "        return x.last_hidden_state[:,0,:]\n",
    "    \n",
    "    def forward(self, sent, pos):\n",
    "        # Forward\n",
    "        sent=self.pretrained(sent)\n",
    "        pos=self.pretrained(pos)\n",
    "        \n",
    "        # Pooling\n",
    "        repr_sent=self.pooler(sent)\n",
    "        repr_pos=self.pooler(pos)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        sim=self.cos_sim(repr_sent.unsqueeze(1), repr_pos.unsqueeze(0))/self.temp\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        label=torch.arange(sim.size(0)).long().to(sim.device)\n",
    "        loss=self.loss(sim, label)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3acf7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model: Unsupervised SimCSE\n",
    "model=SimCSE(pretrained=pretrained).to(device)\n",
    "model.train()\n",
    "\n",
    "# Optimizer, Scheduler\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=int(epochs*len(dataset_train)/(accum_steps*batch_size))\n",
    ")\n",
    "\n",
    "# Mixed Precision: GradScaler\n",
    "scaler=amp.GradScaler()\n",
    "\n",
    "# Tensorboard\n",
    "writer=SummaryWriter()\n",
    "\n",
    "step_global=0\n",
    "for epoch in range(epochs):\n",
    "    _loss=0\n",
    "    optimizer.zero_grad()\n",
    "    for step, (sent, pos) in enumerate(dataloader_train):\n",
    "        # Load on Device\n",
    "        sent=sent.to(device)\n",
    "        pos=pos.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        with amp.autocast():\n",
    "            loss=model(sent, pos)\n",
    "            loss=loss/accum_steps\n",
    "        # Backward\n",
    "        scaler.scale(loss).backward()\n",
    "        _loss+=loss.item()\n",
    "        \n",
    "        # Step\n",
    "        if (step+1)%accum_steps==0:\n",
    "            step_global+=1\n",
    "            \n",
    "            # Tensorboard\n",
    "            writer.add_scalar(\n",
    "                f'loss_train/SimCSE_Unsup_batch{int(accum_steps*batch_size)}_lr{lr}_epochs{epochs}',\n",
    "                _loss,\n",
    "                step_global\n",
    "            )\n",
    "            _loss=0\n",
    "            \n",
    "            # Optimizer, Scheduler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if (step_global+1)%250==0:\n",
    "                # Eval Phase\n",
    "                # Save Model\n",
    "                model.to(torch.device('cpu'))\n",
    "                torch.save(\n",
    "                    model,\n",
    "                    f'../model/SimCSE_Unsup_batch{int(accum_steps*batch_size)}_lr{lr}_step{step_global+1}'\n",
    "                )\n",
    "                model.to(device)\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5c777",
   "metadata": {},
   "source": [
    "### Evaluation on STS-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1384b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e2cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Unsupervised SimCSE\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-Trained LM\n",
    "        self.pretrained=pretrained\n",
    "        # Pooling Layer: MLP (Train Only)\n",
    "        self.mlp=nn.Linear(self.pretrained.config.hidden_size, self.pretrained.config.hidden_size)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        self.cos_sim=nn.CosineSimilarity(dim=-1)\n",
    "        # Temperature\n",
    "        self.temp=0.05\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        self.loss=nn.CrossEntropyLoss()\n",
    "        \n",
    "    def pooler(self, x):\n",
    "        # [CLS] with MLP (Train Only)\n",
    "        x=x.last_hidden_state[:,0,:]\n",
    "        return self.mlp(x)\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        # Return Sentence Representation\n",
    "        x=self.pretrained(x)\n",
    "        return x.last_hidden_state[:,0,:]\n",
    "    \n",
    "    def forward(self, sent, pos):\n",
    "        # Forward\n",
    "        sent=self.pretrained(sent)\n",
    "        pos=self.pretrained(pos)\n",
    "        \n",
    "        # Pooling\n",
    "        repr_sent=self.pooler(sent)\n",
    "        repr_pos=self.pooler(pos)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        sim=self.cos_sim(repr_sent.unsqueeze(1), repr_pos.unsqueeze(0))/self.temp\n",
    "        \n",
    "        # Contrastive Loss\n",
    "        label=torch.arange(sim.size(0)).long().to(sim.device)\n",
    "        loss=self.loss(sim, label)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccac511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device=torch.device(\"cuda:2\")\n",
    "\n",
    "# Pre-Trained Tokenizer\n",
    "tokenizer=RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load Trained Model: Supervised SimCSE\n",
    "model=torch.load(\"../model/SimCSE_Unsup_batch128_lr5e-05_step2500\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec74df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STS Benchmark Dataset\n",
    "# https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark\n",
    "with open(\"../dataset/stsbenchmark/sts-test.csv\", \"r\") as f:\n",
    "    stsb_test=f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93b8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Mode\n",
    "model.eval()\n",
    "\n",
    "preds=[]\n",
    "labels=[]\n",
    "for data in stsb_test.split(\"\\n\")[:-1]:\n",
    "    label, sent1, sent2=data.split(\"\\t\")[4:7]\n",
    "    labels.append(float(label))\n",
    "    \n",
    "    repr_sent1=model.get_embedding(tokenizer.encode(sent1, return_tensors=\"pt\").to(device))\n",
    "    repr_sent2=model.get_embedding(tokenizer.encode(sent2, return_tensors=\"pt\").to(device))\n",
    "    \n",
    "    pred=1-spatial.distance.cosine(np.array(repr_sent1.detach().cpu()), np.array(repr_sent2.detach().cpu()))\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f125ba23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.80130688],\n",
       "       [0.80130688, 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d03a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.7889213716498475, pvalue=1.6754146165373438e-293)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910801f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
